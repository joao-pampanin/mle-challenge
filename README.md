# MLE Challenge, Property-Friends Real State case

> **_NOTE:_**  My commit history is clearly unnatural. This happened because I did the challenge in another Github account, and only at the end discovered that my account was flagged for a reason unknown to me. Since Gitub Support may take weeks to answers me, I decided it was better do replicate my repo in this account (forking or changing ownership of the repo was not an option for a flagged account)

## Overview
This project is the deliverable for the Property-Friends Real Estate case challenge. It includes a machine learning training pipeline with data preprocessing, model training and evaluation steps. It also includes an API for making predictions.

## Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Assumptions and Improvements](#assumptions-and-improvements)

## Project Structure
    .
    ├── data
    │   ├── 00_logs                # Logs generated by the API
    │   ├── 01_raw                 # Train and Test raw files
    │   ├── 02_processed           # Preprocessor file
    │   └── 03_model               # Model file
    ├── examples
    │   ├── api_consumption        # Python and Bash examples on API usage
    ├── src
    │   ├── __init__.py
    │   ├── api.py                 # FastAPI implementation for predict
    │   ├── config.py              # Configuration for Paths and Model/API Params
    │   ├── data_preprocessing.py  # Pre Processing of the Train and Test files 
    │   ├── main.py                # Pipeline runner
    │   ├── model_evaluation.py    # RMSE, MAPE and MAE evaluation
    │   ├── model_training.py      # GradientBoostingRegressor training
    ├── .gitignore
    ├── docker-compose.yml
    ├── Dockerfile
    ├── README.md
    └── requirements.txt

## Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/joao-pampanin/mle-challenge.git
    cd mle-challenge
    ```

> **_NOTE:_**  Installation steps 2 and 3 are only necessary to run and debug the code manually. To run the pipeline and API with Docker, go to Usage.

2. Create and activate a virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Usage
> **_NOTE:_**  The steps below should be run inside the cloned repository folder "mle-challenge". See Installation.

> Since the data files are not on GitHub, make sure to add the train and test files before running the Training pipeline. Because of the lack of data, the Training pipeline must be ran before the API. 
### Training
1. Run the following command:
    ```bash
    docker compose up train --build
    ```
### API
1. Run the following command:
    ```bash
    docker compose up api --build
    ```

## API Documentation
## Structure
The API provides endpoints for making predictions. It receives the following arguments:

- **Predict Endpoint**
  - **URL**: /predict
  - **Method**: POST
  - **Query Parameter**: api_key
  - **Request Body**:
    ```json
    {
      "type": "property_type",
      "sector": "property_sector",
      "net_usable_area": 123.0,
      "net_area": 123.0,
      "n_rooms": 1.0,
      "n_bathroom": 1.0,
      "latitude": -10.1010,
      "longitude": -10.1010
    }
    ```
  - **Response**:
    ```json
    {
      "prediction": 12345.12
    }
    ```

## Usage examples
Functional examples of requests to this API can be found in the examples/api_consumption folder. There are two examples available:
1. **api_request_curl.sh**: Use through the Unix Terminal (Tested on Linux only).
2. **api_request_python.py**: Use directly from python with the requests library
> NOTE: The requests library is not present in the [requirements.txt](http://_vscodecontentref_/0) file since it is not part of the main project, so it needs to be installed manually for this script to work. 

## Assumptions and Improvements
### Assumptions
1. **Code change**: The pipeline was created while trying to make as little change as possible to the Data Science original structure. Changes were made only to transform the Jupyter notebook into the pipeline itself and best-practice changes, like Docstrings, Linting, and Typing.

2. **Database abstraction**: Abstraction can relate to the "Abstract Methods" concept of OOP. I assumed that it wasn't useful to implement it this way since my pipeline doesn't use OOP in general (except for the minor class usage in the config.py file). The way I interpreted it was to just "make it really easy to change the data source", and in the current code it can be done by just changing the source at lines 21 and 22 of the load_data method inside the data_preprocessing.py file.

### Improvements
1. **Pre-commit**: For linting, adding a pre-commit hook is helpful. Since this was just a small delivery, the linters Black and Isort were used manually.

2. **Data Validation**: Basic validation at least to guarantee that the data format is correct is important. One even further step to validate data would be to run the input with a tool like Great Expectations.

3. **Logging**: The existing logging is quite superficial and only captures time, input, and output of predictions. For example, to analyze the API reliability, metrics like request time can help a lot. Some training metrics could be logged as well to track model performance over time.

4. **Testing**: Testing is a good practice, and this code does not have tests currently.

5. **API Key Security**: The API Key is hardcoded in the config.py file. For a real project, better security practices should be taken.
